# ğŸ§  Next Word Predictor using PyTorch RNN

This project implements a **Next Word Prediction** model trained on a custom text dataset using a **Recurrent Neural Network (RNN)** built with PyTorch.  
The goal is to predict the most likely next word in a given sequence, demonstrating language modeling and sequence prediction concepts.

---

## ğŸš€ Features
- Preprocesses and tokenizes raw text data  
- Builds a custom vocabulary dynamically  
- Implements a PyTorch-based Recurrent Neural Network (RNN)  
- Trains the model using cross-entropy loss  
- Predicts the next possible word(s) given a sentence  
- Supports GPU (CUDA) acceleration for faster training  
- Provides clean and reusable modular code structure  

---

## ğŸ“‚ Dataset
The dataset (`next_word_predictor.txt`) contains plain English text used to train the model for sequence-based next-word prediction.  
Each line or paragraph contributes to building the word vocabulary and learning contextual patterns.

**Example Data Snippet:**
```
I love to eat apples
I love to play cricket
I love to read books
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/ashsus09/Next-Word-Predictor-PyTorch-RNN.git
cd next-word-predictor
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Training Script
```bash
python train.py
```

**requirements.txt**
```
torch
numpy
pandas
matplotlib
```

---

## ğŸ§  Model Architecture

| Layer | Description |
|-------|--------------|
| **Embedding Layer** | Converts words into dense vector representations |
| **RNN Layer** | Learns sequential relationships between words |
| **Fully Connected Layer** | Outputs probabilities for each word in the vocabulary |

---

## ğŸ“Š Training Summary

| Epoch | Loss | Observation |
|--------|-------|-------------|
| 1 | 7.10 | Model begins random predictions |
| 10 | 3.65 | Learns basic structure |
| 20 | 2.22 | Recognizes frequent word patterns |
| 30 | 1.68 | Predicts contextually accurate next words |

---

## ğŸ¤– Prediction Examples

**Input â†’ Output Predictions**
```
"the sun was shining" â†’ ['brightly', 'through', 'over']
"deep in the forest" â†’ ['darkness', 'silence', 'trees']
"quantum computing is" â†’ ['revolutionizing', 'changing', 'advancing']
"she walked into the" â†’ ['room', 'house', 'city']
```

---

## ğŸ§© Model Evaluation

- **Loss Function:** CrossEntropyLoss  
- **Optimizer:** Adam (lr=0.001)  
- **Gradient Clipping:** max_norm = 5 (to prevent exploding gradients)  
- **Training Device:** CPU / GPU (Auto-detected)

---

## ğŸ”® Future Improvements

- Upgrade model to **LSTM or GRU** for improved long-term dependency learning  
- Integrate pre-trained embeddings like **GloVe** or **Word2Vec**  
- Build a **Streamlit web interface** for interactive word prediction  
- Expand training dataset for richer vocabulary and better generalization  
- Compare with Transformer-based models (e.g., GPT-like architecture)

---

## ğŸ§° Tech Stack

**Language:** Python  
**Framework:** PyTorch  
**Tools:** NumPy, Pandas, Matplotlib  
**Model:** Recurrent Neural Network (RNN)

---

## ğŸ‘¨â€ğŸ’» Author

**Aastha**  
GitHub: @ashsus09
